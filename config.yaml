general:
    dataset_root: "datasets/development"
    seed: 42
    relaxation: "godel"
    run_eval_mode: False

training:
    epochs: 20
    batch_size: 64
    shuffle: True
    num_workers: 4
    use_cuda: True
    optimizer:
        step_rule: "adam"
        learning_rate: 0.0002
        clip_grad_norm: 0.65
        weight_decay: 0.000000001
   
logging:
    save_checkpoint: True # save a checkpoint every epoch
    experiment_tag: 'development_run'
    load_checkpoint: False # use a saved checkpoint
    resume_training: True # if false, discard everything except model weights
    load_from_tag: 'your_previous_experiment_model' # if you want to load from prev experiment

model:
    embedding_dim: 128
    mlp_hidden_layers: 3
    mlp_activation: "relu"
    mlp_dropout: 0.0
    lstm_iters: 16
    lstm_activation: "tanh"
    lstm_dropout: 0.0
    attention_nheads: 4
    attention_pdrop: 0.0
    attention_resid_pdrop: 0.0

